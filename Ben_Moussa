import os
import ssl
import nltk
import spacy
import rdflib
import os
import subprocess
import pandas as pd
import numpy as np
from nltk.corpus import wordnet
from rdflib import Graph, URIRef, Literal, BNode, Namespace
from rdflib.namespace import FOAF, RDF




def get_tuples_from_noun_chunk(noun,tuples,call="main"):
    # Vorbereitung:
    # if call == "sub":
#    print("the taken noun is: {}".format(noun))
    nlp = spacy.load("en_core_web_md")
    invalide_adjs = ["-", "various", "several", "much", "many"]
    invalide_advs = ["-", "as", "several times", "several", "much", "many"]
    determiners = ["all", "a", "an", "the", "some",#"and",
                   "several", "many", "much", "more", "-"]
    deleted = []
    if call == "main":
        noun_to_list = []
        for word in noun:
            if (word.text.lower() in determiners) or (
                    word.head.text.lower() in determiners):
                deleted.append(word.text.lower())
            else:
                noun_to_list.append(word)
        noun = noun_to_list
    if noun[-1].pos_ == "NOUN":
        head = noun[-1]
    else:
        return
    ind = 0
    mensioned = []
    adjectives = []
    adverbes = []
    conj_chunks = {}
    for word in reversed(noun):
        ind -= 1
        # print(word, "\t", word.pos_, "\t", word.dep_,
        #       "\t", word.tag_, "\t", word.head.text)

        if word == head:
            compound = [head]
            continue        

        # Bearbeitung der compounds teilweise bestehend aus NOUNs:
        elif (word.pos_ == "NOUN") and (word.head == head) and (
                word.dep_ in ["compound","nmod"]): #and (1 == 0):
            # children = [child for child in word.children]
            new_chunk = []
            children = []
            for child in word.children:
                if child in noun:
                    children.append(child)
                                        
        # Komplexe COMPUNDs:
            if len(children) > 0:
                for part in noun:
                    if (part in list(word.lefts)):
                        new_chunk.append(part)
                new_chunk = new_chunk + [word]
#                print("the given chunk is: {}".format(new_chunk))
                get_tuples_from_noun_chunk(new_chunk, tuples, call="sub")
                object_compound = ""
                subject_compound = ""
                for part in compound:
                    object_compound = part.text.lower() +" "+ object_compound
                for part in new_chunk:
                    subject_compound = subject_compound +" "+ part.text.lower() 
                
                subject_compound = subject_compound +" "+ object_compound
                tuples.append(( subject_compound, RDF.type, object_compound ))
#                for part in reversed(new_chunk):
#                    compound.append(part)

      # Einfache COMPOUNDs:
#            elif (word.pos_ == "NOUN"):
            else:
                subject_compound = ""
                object_compound = ""
                for part in noun[ind:]:
                    subject_compound = subject_compound + " " + part.text.lower()
                    if part == noun[ind]:
                        continue
                    else:
                        object_compound = object_compound + " " + part.text.lower()
                tuples.append((subject_compound,
                               RDF.type, object_compound))
#                compound.append(word)
                new_chunk = [word]
                mensioned.append(word)
                
                
                
        # Erweiterung der NOUNs (in Bearbeitung!!!)
            if (len(word.conjuncts) > 0) and (str(word.i) in list(conj_chunks.keys())):
                conj_chunks[str(main_conj.i)].append(new_chunk)
                hidden_chunks = []
                for chunk_list in conj_chunks[str(word.i)]:#!!!!!!!!!!!!!!!!!!!!!!!!!
                    hidden_chunks = hidden_chunks + chunk_list
                
                subject_compound = ""
                object_compound = ""
                for part in hidden_chunks:
                    subject_compound += " "+ part.text.lower()
                for part in reversed(compound):
                    object_compound += " "+ part.text.lower()
                    subject_compound += " "+ part.text.lower()
                tuples.append(( subject_compound, RDF.type, object_compound ))
                for chunk_list in conj_chunks[str(word.i)]:
#                    print("chunk_list = {}!!!!!!!!!!!!!".format(chunk_list,type(chunk_list)))
                    object_compound1 = ""
                    if chunk_list[0].pos_ not in ["CCONJ","PUNCT"]:
                        for element in chunk_list:
                            object_compound1 += " "+ element.text.lower()
                        object_compound1 += " "+ object_compound
                        tuples.append(( subject_compound, "property: related to",
                                       object_compound1))
                for part in reversed(hidden_chunks):
                    compound.append(part)
                        
                        
            else:
                for part in reversed(new_chunk):
                    compound.append(part)
                    
                
                
                
                
                
                
                
        # Berarbeitung der ADJECTIVES und der ADVERBES:
        elif ((word.pos_ == "ADJ" or word.dep_ == "amod") and (word.head == head) and (word.text.lower()
              not in invalide_adjs)) or ((word.pos_ == "ADV") and (
              word.head in adjectives) and (word.text.lower() not in invalide_advs)):
            object_compound = ""
            ontological_property = word.text
            for part in reversed(compound):
                object_compound += " {}".format(part)
            subject_compound = ontological_property + " " + object_compound

            # tuples.append(( BNode(subject_compound),RDF.type,object_compound ))
            tuples.append((subject_compound, RDF.type, object_compound))
            # tuples.append(( BNode(subject_compound),n["has property"],ontological_property ))
            tuples.append((subject_compound, "has property", ontological_property))
        
        
        # Erweiterung der Adjektive und Adverbien:            
            if (len(word.conjuncts)>0) and (
                    str(word.i) in list(conj_chunks.keys())):
                conj_chunks[str(main_conj.i)].append([word])
                hidden_chunks = []
                for adj_list in conj_chunks[str(word.i)]:
                    hidden_chunks = hidden_chunks + adj_list
                subject_compound = ""
                object_compound = ""
                ontological_property = ""
                for part in hidden_chunks:
                    ontological_property += " "+ part.text.lower()
                    subject_compound += " "+ part.text.lower()
                for part in reversed(compound):
                    object_compound += " "+ part.text.lower()
                    subject_compound += " "+ part.text.lower()
                tuples.append(( subject_compound, RDF.type,object_compound))
                tuples.append((subject_compound, "has property",ontological_property))
                
                for adj_list in conj_chunks[str(word.i)]:
                    object_compound1 = ""
                    ontological_property = ""
                    if adj_list[0].pos_ not in ["CCONJ","PUNCT"]:
                        for element in adj_list:
                            ontological_property += " "+ element.text.lower()
                        object_compound1 = ontological_property +" "+ object_compound
                        tuples.append((subject_compound,RDF.type,object_compound1))
                        tuples.append((subject_compound,"has property",ontological_property))
                
                for part in reversed(hidden_chunks):
                    compound.append(part)
                if word.pos_ == "ADJ" or word.dep_ == "amod":
                    adjectives.append(word)
                elif word.pos_ == "ADV":
                    adverbes.append(word)
            else:
                compound.append(word)
                if word.pos_ == "ADJ" or word.dep_ == "amod":  
                    adjectives.append(word)
                elif word.pos_ == "ADV":
                    adverbes.append(word)                
                
                
                
                

        # CONJ (In Bearbeitung !!!)
        elif word.dep_=="conj":
            conj_children = []
            for part in noun:
                if part in list(word.lefts):
                    conj_children.append(part)
            new_chunk = conj_children + [word]
            subject_compound = ""
            object_compound = ""
            for part in new_chunk:
                subject_compound = subject_compound +" "+ part.text.lower()            
            for part in reversed(compound):
                subject_compound = subject_compound +" "+ part.text.lower()
                object_compound = object_compound +" "+ part.text.lower()
            if len(conj_children) > 0:
                get_tuples_from_noun_chunk(new_chunk,tuples,call="sub")
            for conj in word.conjuncts:
                if conj.dep_ != "conj":
                    main_conj = conj
                    if str(main_conj.i) not in list(conj_chunks.keys()):
                        conj_chunks[str(main_conj.i)] = []
                    break
            # if the conj in a noun:
            if (main_conj.pos_=="NOUN") and (main_conj.head == head) and (
                    main_conj.dep_ in ["compound","nmod"]):
                tuples.append(( subject_compound, RDF.type, object_compound ))
                conj_chunks[str(main_conj.i)].append(new_chunk)
            # if the conj in an adjective or an adverb:            
            elif ((main_conj.pos_ == "ADJ" or main_conj.dep_ == "amod") and (
                    main_conj.head == head) and (main_conj.text.lower() 
                    not in invalide_adjs)) or ((main_conj.pos_ == "ADV") and (
              main_conj.head in adjectives) and (main_conj.text.lower() not in
                                           invalide_advs)):
                ontological_property = word.text.lower()
                tuples.append(( subject_compound, RDF.type, object_compound ))
                tuples.append(( subject_compound,"has property",ontological_property))
                conj_chunks[str(main_conj.i)].append(new_chunk)
                if word.pos_ == "ADJ" or word.dep_ == "amod":  
                    adjectives.append(word)
                elif word.pos_ == "ADV":
                    adverbes.append(word)
                
                
                
        # CC (In Bearbeitung !!!)                
        elif word.pos_ in ["CCONJ","PUNCT"]:
            conj_chunks[str(main_conj.i)].append([word])
            
            



def visualize(doc, style="dep"):
    html = spacy.displacy.render(doc,style=style)
    f = open("Z:\\BA\\BA-main\\BA-main\\Alte_Python.files\\Text_Files\\datavis.html","w")
    f.write(html)
    f.close()
    print("Z:\\BA\\BA-main\\BA-main\\Alte_Python.files\\Text_Files")
#    subprocess.run("call C:\\Users\\user\\datavis.html",shell=True, 
#                   capture_output=True)



def show_graph(graph, m=-1, formatt="enumerative"):
    if formatt == 'ttl':
        print(graph.serialize(format == "ttl"))
    elif m == -1:
        for index, (sub, pred, obj) in enumerate(graph):
            print(sub, "\n", pred, "\n", obj, "\n\n")
    else:
        for index, (sub, pred, obj) in enumerate(graph):
            print("{sub<10}\n{pred<10}\n{obj<10}\n\n")
            if index >= m:
                break


def show_spacy_dependencies(doc):
    for tocken in doc:
        print("{}\t{}\t{}".format(tocken.lemma_,
              tocken.dep_, spacy.explain(tocken.dep_)))
    print("\n")

    # g = Graph()
    # g.bind("foaf",FOAF)
    # n = Namespace("http://example.org/")
    # all_dep = set_predicat(sents[0],n, g)
    # print(all_dep,"\n")
    # # for tock, dep in all_dep.items():
    # if "attr" in all_dep.keys():
    #     g.add((BNode(all_dep["nsubj"].text), RDF.type, BNode(all_dep["attr"].text)))
    # elif "dobj"  in all_dep.keys():
    #     g.add(( BNode(all_dep["nsubj"].text), n[all_dep["ROOT"].lemma_] ,BNode(all_dep["dobj"].text) ))


def doc_to_df(sent):
    sent_df = pd.DataFrame(index=["text", "dep", "tag", "pos", "direct_head", "direct_head_position",
                           "head_position", "root_verb"], columns=[a for a in range(len(sent))])
    clauses = 0
    verb_pos = []
    verb_pos_all = []
    for i in sent_df.columns.values:
        sent_df[i].loc["text"] = sent[i].text
        sent_df[i].loc["tag"] = sent[i].tag_
        sent_df[i].loc["pos"] = sent[i].pos_
        sent_df[i].loc["dep"] = sent[i].dep_
        sent_df[i].loc["head_position"] = sent.to_json()["tokens"][i]["head"]
        sent_df[i].loc["direct_head_position"] = sent_df[i].loc["head_position"]
        sent_df[i].loc["direct_head"] = sent[sent.to_json()["tokens"][i]
                                             ["head"]].text
        sent_df[i].loc["root_verb"] = sent[i].head.text
        if (sent_df[i].loc["dep"] in ["relcl", "advcl", "ccomp", "acl"]) or (sent_df[i].loc["dep"] == "ROOT"):
            clauses += 1
            verb_pos.append(i)
            verb_pos_all.append(i)
        if (sent_df[i].loc["dep"] == "conj") and (
                "VB" in sent_df[i].loc["tag"]) and (
                sent_df[i].loc["direct_head_position"] in verb_pos):
            verb_pos_all.append(i)
            verb_pos.append(i)

    for i in sent_df.columns.values:
        # j = sent_df[i].loc["head_position_position"]
        j = i
        while sent_df[j].loc["head_position"] not in verb_pos:
            j = sent_df[j].loc["head_position"]
        if not (j in verb_pos):
            j = sent_df[j].loc["head_position"]
        sent_df[i].loc["head_position"] = j
        sent_df[i].loc["root_verb"] = sent[j].text

    for i in verb_pos:
        sent_df[i].loc["head_position"] = i
        sent_df[i].loc["root_verb"] = sent[i].text
    # print(sent_df)
    return sent_df
                    

def set_predicat(sent, namespace, graph=rdflib.Graph):
    all_dep = {}
    for tock in sent:
        # if tock.dep_ not in list(all_dep.values()):
        if tock.dep_ in ["nsubj", "ROOT", "attr", "dobj"]:
            all_dep[str(tock.dep_)] = tock

    return all_dep





""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
nlp = spacy.load("en_core_web_md")
g = Graph()
g.bind("foaf", FOAF)


# Abrufen des Textes (in Bearbeitung)
# f = open("C:\\Users\\smmnbenm\\Documents\\GitHub\\BA\\Alte_Python.files\\Text_Files\\pdfs6.txt", "r")
f = open("Z:\\BA\\BA-main\\BA-main\\Alte_Python.files\\Text_Files\\pdfs6.txt","r")
Text = f.read()
f.close()
sp_text = nlp(Text)
sents = [s for s in sp_text.sents]
all_dep = {}
sent_o = sents[0]


# Andere Satzbeispiele
sent1 = nlp(sents[0].text)
sent2 = nlp("Ultra-soft pseudopotentials are used to represent the ionic cores, allowing for a reasonable treatment of first-row atoms and transition metals even with a relatively limited plane wave basis, that enhances the observed angular modules")
sent3 = nlp("The largest problem that bootstrapping methods have to face is error-propagation: misclassified items will lead to the acquisition of even more misclassified items. Various attempts have been made to minimize this thread")
sent4 = nlp("this morning, Sam and Antoni have had lunch and dinner at the school courtyard, where the professors and the rest of the school stuff were also to be found during the big summer and spring times")
#sent4 = nlp("struggle, Sam and Antoni have had lunch and dinner at the school courtyard, where the professors and the rest of the school stuff were also to be found during the big summer and spring times")
sent6 = nlp("Manfred, Sam and Antoni have had lunch and dinner at the school courtyard, where the professors and the rest of the school stuff were also to be found during the big summer and spring times")
# sent3 = nlp("The largest problem that bootstrapping methods have to face is error-propagation and error-mismatch: misclassified items will lead to the acquisition of even more misclassified items. Various attempts have been made to minimize this thread")
sent5 = nlp("Ammonia and carbon dioxide emissions are to be avoided by any means")
sent7 = nlp("the nice and committed man has experiences his first defeat")
sent8 = nlp("The nicely and thoroughly sliced piece of cake is on the table")
sent = nlp("Tim and Alfred look for words occurring together in syntactical formations that involve full parsing of the corpus")
sent = sent8

# Überfürung der doc-Variablen in DataFrame-Variable zur Veranschaulichung 
# der Haupt- und Nebensätze
df = doc_to_df(sent)

# Erstellung aller möglichen Tuples nur anhand der "noun chunks"
# 1- Extraction der chunks
nouns = [noun for noun in sent.noun_chunks]
for noun in nouns:
    if noun.text.lower() in ["that", "which", "-"]:
        nouns.remove(noun)
nouns_text = [noun.text for noun in nouns]

# 2- Erstellung aller Tuples aus einem Satz (Funktion in Bearbeitung):
tuples = []
for noun in (nouns):
    print(noun)
    get_tuples_from_noun_chunk(noun,tuples, call="main")
